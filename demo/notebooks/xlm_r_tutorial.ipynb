{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-R Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will train(fine-tune), eval and export a XLM-R model based on a large pre-train model.\n",
    "\n",
    "We are using a dummy dataset with test config so we can train the model in a few minutes\n",
    "\n",
    "See the full paper and introduction in https://pytext.readthedocs.io/en/master/xlm_r.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install PyText from source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following https://pytext.readthedocs.io/en/master/installation.html#install-from-source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide 2 pre-trained models \"xlmr.base.v0\" and \"xlmr.large.v0\" model at https://pytext.readthedocs.io/en/master/xlm_r.html#pre-trained-models\n",
    "\n",
    "Let's download \"xlmr.large.v0\" and extract the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.v0.tar.gz\n",
    "!tar xzf ./xlmr.large.v0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Config and Tran Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from pytext import workflow\n",
    "from pytext.config.serialize import pytext_config_from_json\n",
    "from pytext.models.roberta import RoBERTa\n",
    "from pytext.task.serialize import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /data/users/stevenliu/notebooks/dummy_train_file.txt\n",
      "Created: /data/users/stevenliu/notebooks/dummy_test_file.txt\n",
      "Created: /data/users/stevenliu/notebooks/dummy_eval_file.txt\n"
     ]
    }
   ],
   "source": [
    "dummy_dataset = \"\"\"contradiction\\t▁Well , ▁I ▁wasn ' t ▁even ▁thinking ▁about ▁that , ▁but ▁I ▁was ▁so ▁frustra ted , ▁and , ▁I ▁ended ▁up ▁talking ▁to ▁him ▁again .\\t▁I ▁have nt ▁spoke n ▁to ▁him ▁again .\\nentailment\\t▁Well , ▁I ▁wasn ' t ▁even ▁thinking ▁about ▁that , ▁but ▁I ▁was ▁so ▁frustra ted , ▁and , ▁I ▁ended ▁up ▁talking ▁to ▁him ▁again .\\t▁I ▁was ▁so ▁up set ▁that ▁I ▁just ▁started ▁talking ▁to ▁him ▁again .\\nneutral\\t▁Well , ▁I ▁wasn ' t ▁even ▁thinking ▁about ▁that , ▁but ▁I ▁was ▁so ▁frustra ted , ▁and , ▁I ▁ended ▁up ▁talking ▁to ▁him ▁again .\\t▁We ▁had ▁a ▁great ▁talk .\\nneutral\\t▁And ▁I ▁thought ▁that ▁was ▁a ▁privilege , ▁and ▁it ' s ▁still , ▁it ' s ▁still , ▁I ▁was ▁the ▁only ▁ni ne ▁two - two ▁Ex - O ▁which ▁was ▁my ▁AFF C ▁Air ▁Force ▁Care er ▁field .\\t▁I ▁was ▁not ▁aware ▁that ▁I ▁was ▁not ▁the ▁only ▁person ▁to ▁be ▁at ▁the ▁field ▁that ▁day .\\nentailment\\t▁And ▁I ▁thought ▁that ▁was ▁a ▁privilege , ▁and ▁it ' s ▁still , ▁it ' s ▁still , ▁I ▁was ▁the ▁only ▁ni ne ▁two - two ▁Ex - O ▁which ▁was ▁my ▁AFF C ▁Air ▁Force ▁Care er ▁field .\\t▁I ▁was ▁under ▁the ▁impression ▁that ▁I ▁was ▁the ▁only ▁one ▁with ▁that ▁number ▁at ▁the ▁AFF C ▁Air ▁Force ▁Care er ▁field .\\ncontradiction\\t▁And ▁I ▁thought ▁that ▁was ▁a ▁privilege , ▁and ▁it ' s ▁still , ▁it ' s ▁still , ▁I ▁was ▁the ▁only ▁ni ne ▁two - two ▁Ex - O ▁which ▁was ▁my ▁AFF C ▁Air ▁Force ▁Care er ▁field .\\t▁We ▁all ▁were ▁given ▁the ▁same ▁exact ▁number ▁no ▁matter ▁what ▁privilege s ▁we ▁were ▁promise d ▁to ▁be ▁gran ted , ▁it ▁was ▁all ▁a ▁lie .\\ncontradiction\\t▁They ▁told ▁me ▁that , ▁uh , ▁that ▁I ▁would ▁be ▁called ▁in ▁a ▁guy ▁at ▁the ▁end ▁for ▁me ▁to ▁meet .\\t▁I ▁was ▁never ▁told ▁anything ▁about ▁meeting ▁anyone .\\nentailment\\t▁They ▁told ▁me ▁that , ▁uh , ▁that ▁I ▁would ▁be ▁called ▁in ▁a ▁guy ▁at ▁the ▁end ▁for ▁me ▁to ▁meet .\\t▁I ▁was ▁told ▁a ▁guy ▁would ▁be ▁called ▁in ▁for ▁me ▁to ▁meet .\\nneutral\\t▁They ▁told ▁me ▁that , ▁uh , ▁that ▁I ▁would ▁be ▁called ▁in ▁a ▁guy ▁at ▁the ▁end ▁for ▁me ▁to ▁meet .\\t▁The ▁guy ▁showed ▁up ▁a ▁bit ▁late .\\ncontradiction\\t▁There ' s ▁so ▁much ▁you ▁could ▁talk ▁about ▁on ▁that ▁I ' ll ▁just ▁skip ▁that .\\t▁I ▁want ▁to ▁tell ▁you ▁everything ▁I ▁know ▁about ▁that !\\n\"\"\"\n",
    "\n",
    "dummy_train_filename = os.path.join(os.getcwd(), \"dummy_train_file.txt\")\n",
    "dummy_test_filename = os.path.join(os.getcwd(), \"dummy_test_file.txt\")\n",
    "dummy_eval_filename = os.path.join(os.getcwd(), \"dummy_eval_file.txt\")\n",
    "for filename in (dummy_train_filename, dummy_test_filename, dummy_eval_filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(dummy_dataset)\n",
    "        print(f\"Created: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To train a real model,set your own dataset here\n",
    "TRAIN_FILENAME = dummy_train_filename\n",
    "TEST_FILENAME = dummy_test_filename\n",
    "EVAL_FILENAME = dummy_eval_filename\n",
    "\n",
    "PRE_TRAIN_MODEL_DIR = os.path.join(os.getcwd(), \"xlmr.large.v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_json = \"\"\"\n",
    "   {\n",
    "    \"version\": 18,\n",
    "\t\"task\": {\n",
    "\t\t\"DocumentClassificationTask\": {\n",
    "\t\t\t\"data\": {\n",
    "\t\t\t\t\"Data\": {\n",
    "\t\t\t\t\t\"source\": {\n",
    "\t\t\t\t\t\t\"TSVDataSource\": {\n",
    "\t\t\t\t\t\t\t\"train_filename\": \"{TRAIN_FILENAME}\",\n",
    "\t\t\t\t\t\t\t\"test_filename\": \"{TEST_FILENAME}\",\n",
    "\t\t\t\t\t\t\t\"eval_filename\": \"{EVAL_FILENAME}\",\n",
    "\t\t\t\t\t\t\t\"field_names\": [\n",
    "\t\t\t\t\t\t\t\t\"label\",\n",
    "\t\t\t\t\t\t\t\t\"text1\",\n",
    "\t\t\t\t\t\t\t\t\"text2\"\n",
    "\t\t\t\t\t\t\t]\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"batcher\": {\n",
    "\t\t\t\t\t\t\"Batcher\": {\n",
    "\t\t\t\t\t\t\t\"train_batch_size\": 8,\n",
    "\t\t\t\t\t\t\t\"eval_batch_size\": 8,\n",
    "\t\t\t\t\t\t\t\"test_batch_size\": 8\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"sort_key\": \"tokens\"\n",
    "\t\t\t\t}\n",
    "\t\t\t},\n",
    "\t\t\t\"trainer\": {\n",
    "\t\t\t\t\"TaskTrainer\": {\n",
    "\t\t\t\t\t\"epochs\": 1,\n",
    "\t\t\t\t\t\"early_stop_after\": 0,\n",
    "\t\t\t\t\t\"max_clip_norm\": null,\n",
    "\t\t\t\t\t\"report_train_metrics\": true,\n",
    "\t\t\t\t\t\"target_time_limit_seconds\": null,\n",
    "\t\t\t\t\t\"do_eval\": true,\n",
    "\t\t\t\t\t\"num_samples_to_log_progress\": 10,\n",
    "\t\t\t\t\t\"num_accumulated_batches\": 1,\n",
    "\t\t\t\t\t\"num_batches_per_epoch\": 1,\n",
    "\t\t\t\t\t\"optimizer\": {\n",
    "\t\t\t\t\t\t\"Adam\": {\n",
    "\t\t\t\t\t\t\t\"lr\": 0.000005,\n",
    "\t\t\t\t\t\t\t\"weight_decay\": 0\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"scheduler\": null,\n",
    "\t\t\t\t\t\"sparsifier\": null,\n",
    "\t\t\t\t\t\"fp16_args\": {\n",
    "\t\t\t\t\t\t\"FP16OptimizerApex\": {\n",
    "\t\t\t\t\t\t\t\"init_loss_scale\": null,\n",
    "\t\t\t\t\t\t\t\"min_loss_scale\": null\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t},\n",
    "\t\t\t\"model\": {\n",
    "\t\t\t\t\"RoBERTa\": {\n",
    "\t\t\t\t\t\"inputs\": {\n",
    "\t\t\t\t\t\t\"tokens\": {\n",
    "\t\t\t\t\t\t\t\"columns\": [\n",
    "\t\t\t\t\t\t\t\t\"text1\",\n",
    "\t\t\t\t\t\t\t\t\"text2\"\n",
    "\t\t\t\t\t\t\t],\n",
    "\t\t\t\t\t\t\t\"vocab_file\": \"{VOCAB_PATH}\",\n",
    "\t\t\t\t\t\t\t\"tokenizer\": {\n",
    "\t\t\t\t\t\t\t\t\"SentencePieceTokenizer\": {\n",
    "\t\t\t\t\t\t\t\t\t\"sp_model_path\": \"{SP_MODEL_PATH}\"\n",
    "\t\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\t\"max_seq_len\": 256\n",
    "\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\"labels\": {\n",
    "\t\t\t\t\t\t\t\"LabelTensorizer\": {\n",
    "\t\t\t\t\t\t\t\t\"column\": \"label\",\n",
    "\t\t\t\t\t\t\t\t\"allow_unknown\": false,\n",
    "\t\t\t\t\t\t\t\t\"pad_in_vocab\": false,\n",
    "\t\t\t\t\t\t\t\t\"label_vocab\": null\n",
    "\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"encoder\": {\n",
    "\t\t\t\t\t\t\"RoBERTaEncoder\": {\n",
    "\t\t\t\t\t\t\t\"load_path\": null,\n",
    "\t\t\t\t\t\t\t\"save_path\": \"encoder.pt\",\n",
    "\t\t\t\t\t\t\t\"shared_module_key\": null,\n",
    "\t\t\t\t\t\t\t\"embedding_dim\": 1024,\n",
    "\t\t\t\t\t\t\t\"vocab_size\": 250002,\n",
    "\t\t\t\t\t\t\t\"num_encoder_layers\": 24,\n",
    "\t\t\t\t\t\t\t\"num_attention_heads\": 16,\n",
    "\t\t\t\t\t\t\t\"model_path\": \"{PRE_TRAIN_MODEL_PATH}\",\n",
    "\t\t\t\t\t\t\t\"is_finetuned\": false\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"decoder\": {\n",
    "\t\t\t\t\t\t\"load_path\": null,\n",
    "\t\t\t\t\t\t\"save_path\": \"decoder.pt\",\n",
    "\t\t\t\t\t\t\"freeze\": false,\n",
    "\t\t\t\t\t\t\"shared_module_key\": \"DECODER\",\n",
    "\t\t\t\t\t\t\"hidden_dims\": [],\n",
    "\t\t\t\t\t\t\"out_dim\": null,\n",
    "\t\t\t\t\t\t\"activation\": \"gelu\"\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t\"output_layer\": {\n",
    "\t\t\t\t\t\t\"load_path\": null,\n",
    "\t\t\t\t\t\t\"save_path\": null,\n",
    "\t\t\t\t\t\t\"freeze\": false,\n",
    "\t\t\t\t\t\t\"shared_module_key\": null,\n",
    "\t\t\t\t\t\t\"loss\": {\n",
    "\t\t\t\t\t\t\t\"CrossEntropyLoss\": {}\n",
    "\t\t\t\t\t\t},\n",
    "\t\t\t\t\t\t\"label_weights\": null\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t},\n",
    "\t\t\t\"metric_reporter\": {\n",
    "\t\t\t\t\"ClassificationMetricReporter\": {\n",
    "\t\t\t\t\t\"model_select_metric\": \"accuracy\",\n",
    "\t\t\t\t\t\"target_label\": null,\n",
    "\t\t\t\t\t\"text_column_names\": [\n",
    "\t\t\t\t\t\t\"text1\",\n",
    "\t\t\t\t\t\t\"text2\"\n",
    "\t\t\t\t\t],\n",
    "\t\t\t\t\t\"recall_at_precision_thresholds\": []\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    " }\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = json.loads(config_json)\n",
    "config_obj = pytext_config_from_json(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_obj.task.data.source.train_filename = TRAIN_FILENAME\n",
    "config_obj.task.data.source.test_filename = TEST_FILENAME\n",
    "config_obj.task.data.source.eval_filename = EVAL_FILENAME\n",
    "\n",
    "config_obj.task.model.inputs.tokens.tokenizer.sp_model_path = os.path.join(\n",
    "    PRE_TRAIN_MODEL_DIR, \"sentencepiece.bpe.model\"\n",
    ")\n",
    "config_obj.task.model.inputs.tokens.vocab_file = os.path.join(PRE_TRAIN_MODEL_DIR, \"dict.txt\")\n",
    "config_obj.task.model.encoder.model_path = os.path.join(PRE_TRAIN_MODEL_DIR, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters: PyTextConfig:\n",
      "    auto_resume_from_snapshot: False\n",
      "    debug_path: /tmp/model.debug\n",
      "    distributed_world_size: 1\n",
      "    export_caffe2_path: None\n",
      "    export_onnx_path: /tmp/model.onnx\n",
      "    export_torchscript_path: None\n",
      "    include_dirs: None\n",
      "    load_snapshot_path: \n",
      "    modules_save_dir: \n",
      "    random_seed: None\n",
      "    report_eval_results: False\n",
      "    save_all_checkpoints: False\n",
      "    save_module_checkpoints: False\n",
      "    save_snapshot_path: /tmp/model.pt\n",
      "    task: DocumentClassificationTask.Config:\n",
      "        data: Data.Config:\n",
      "            batcher: Batcher.Config:\n",
      "                eval_batch_size: 8\n",
      "                test_batch_size: 8\n",
      "                train_batch_size: 8\n",
      "            in_memory: True\n",
      "            sort_key: tokens\n",
      "            source: TSVDataSource.Config:\n",
      "                column_mapping: {}\n",
      "                delimiter: \t\n",
      "                drop_incomplete_rows: False\n",
      "                eval_filename: /data/users/stevenliu/notebooks/dummy_eval_file.txt\n",
      "                field_names: ['label', 'text1', 'text2']\n",
      "                quoted: False\n",
      "                test_filename: /data/users/stevenliu/notebooks/dummy_test_file.txt\n",
      "                train_filename: /data/users/stevenliu/notebooks/dummy_train_file.txt\n",
      "        metric_reporter: ClassificationMetricReporter.Config:\n",
      "            additional_column_names: []\n",
      "            model_select_metric: ComparableClassificationMetric.ACCURACY\n",
      "            output_path: /tmp/test_out.txt\n",
      "            pep_format: False\n",
      "            recall_at_precision_thresholds: []\n",
      "            target_label: None\n",
      "            text_column_names: ['text1', 'text2']\n",
      "        model: RoBERTa.Config:\n",
      "            decoder: MLPDecoder.Config:\n",
      "                activation: Activation.GELU\n",
      "                dropout: 0.0\n",
      "                freeze: False\n",
      "                hidden_dims: []\n",
      "                layer_norm: False\n",
      "                load_path: None\n",
      "                out_dim: None\n",
      "                save_path: decoder.pt\n",
      "                shared_module_key: DECODER\n",
      "            encoder: RoBERTaEncoder.Config:\n",
      "                embedding_dim: 1024\n",
      "                export: False\n",
      "                freeze: False\n",
      "                is_finetuned: False\n",
      "                load_path: None\n",
      "                model_path: /data/users/stevenliu/notebooks/xlmr.large.v0/model.pt\n",
      "                num_attention_heads: 16\n",
      "                num_encoder_layers: 24\n",
      "                output_dropout: 0.4\n",
      "                pooling: PoolingMethod.CLS_TOKEN\n",
      "                save_path: encoder.pt\n",
      "                shared_module_key: None\n",
      "                vocab_size: 250002\n",
      "            inputs: InputConfig:\n",
      "                labels: LabelTensorizer.Config:\n",
      "                    allow_unknown: False\n",
      "                    column: label\n",
      "                    label_vocab: None\n",
      "                    pad_in_vocab: False\n",
      "                tokens: RoBERTaTensorizer.Config:\n",
      "                    base_tokenizer: None\n",
      "                    columns: ['text1', 'text2']\n",
      "                    max_seq_len: 256\n",
      "                    tokenizer: SentencePieceTokenizer.Config:\n",
      "                        sp_model_path: /data/users/stevenliu/notebooks/xlmr.large.v0/sentencepiece.bpe.model\n",
      "                    vocab_file: /data/users/stevenliu/notebooks/xlmr.large.v0/dict.txt\n",
      "            output_layer: ClassificationOutputLayer.Config:\n",
      "                freeze: False\n",
      "                label_weights: None\n",
      "                load_path: None\n",
      "                loss: CrossEntropyLoss.Config:\n",
      "                save_path: None\n",
      "                shared_module_key: None\n",
      "        trainer: TaskTrainer.Config:\n",
      "            do_eval: True\n",
      "            early_stop_after: 0\n",
      "            epochs: 1\n",
      "            fp16_args: FP16OptimizerApex.Config:\n",
      "                init_loss_scale: None\n",
      "                min_loss_scale: None\n",
      "                opt_level: O2\n",
      "            max_clip_norm: None\n",
      "            num_accumulated_batches: 1\n",
      "            num_batches_per_epoch: 1\n",
      "            num_samples_to_log_progress: 10\n",
      "            optimizer: Adam.Config:\n",
      "                eps: 1e-08\n",
      "                lr: 5e-06\n",
      "                weight_decay: 0.0\n",
      "            report_train_metrics: True\n",
      "            scheduler: None\n",
      "            sparsifier: None\n",
      "            target_time_limit_seconds: None\n",
      "    test_out_path: /tmp/test_out.txt\n",
      "    torchscript_quantize: False\n",
      "    use_config_from_snapshot: True\n",
      "    use_cuda_if_available: True\n",
      "    use_deterministic_cudnn: False\n",
      "    use_fp16: False\n",
      "    use_tensorboard: True\n",
      "    version: 18\n",
      "\n",
      "Cuda is not available, running on CPU...\n",
      "\n",
      "        # for debug of GPU\n",
      "        use_cuda_if_available: True\n",
      "        device_id: 0\n",
      "        world_size: 1\n",
      "        torch.cuda.is_available(): False\n",
      "        cuda.CUDA_ENABLED: False\n",
      "        cuda.DISTRIBUTED_WORLD_SIZE: 1\n",
      "        \n",
      "# for debug of FP16: fp16_enabled=False\n",
      "Creating task: DocumentClassificationTask...\n"
     ]
    }
   ],
   "source": [
    "trained_model, best_metric = workflow.train_model(config_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_file = \"/tmp/model.pt\"\n",
    "task, config, _ = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = task.model\n",
    "# model is a torch.nn.Module\n",
    "isinstance(model, torch.nn.Module)"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "429467601069621"
  },
  "disseminate_notebook_info": {
   "bento_version": "20191118-000256",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "deps": [
     "//pytext:main_lib",
     "//pytext/fb:main_lib",
     "//pytext/fb/pur:pur_lib",
     "//fblearner/flow/projects/langtech/pytext_model:types",
     "//configerator/structs/sigrid:model_id-py",
     "//fblearner/predictor/clients:client_lib",
     "//fblearner/predictor/clients/py:api",
     "//fblearner/predictor/clients/tests:model_test_fixtures",
     "//fblearner/predictor/clients/py3:client"
    ],
    "external_deps": []
   },
   "no_uii": true,
   "notebook_number": "176368",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "458983888371875",
   "tags": "",
   "tasks": "",
   "title": "XLM-R Tutorial"
  },
  "kernelspec": {
   "display_name": "pytext",
   "language": "python",
   "name": "bento_kernel_pytext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
