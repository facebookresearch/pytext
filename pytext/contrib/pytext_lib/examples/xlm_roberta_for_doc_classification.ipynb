{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytext Library Prototype: xlm_roberta_for_doc_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T17:56:29.046164Z",
     "start_time": "2020-05-03T17:56:28.844640Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T17:57:55.670165Z",
     "start_time": "2020-05-03T17:57:32.922919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0503 105748.051 cross_entropy.py:26] using fused cross entropy\n"
     ]
    }
   ],
   "source": [
    "import pytext.fb  # monkey patch for internal. e.g. manifold handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune a XLM-Roberta with a task\n",
    "* The task provides high-level APIs and it builds data, model, trainer, metric for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:03:40.712704Z",
     "start_time": "2020-05-03T19:03:40.143901Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytext.contrib.pytext_lib.tasks import XLMRobertaForDocClassificationTask\n",
    "\n",
    "\n",
    "tiny_data_path = \"/mnt/vol/pytext/users/stevenliu/glue_data/CoLA/dev_10.tsv\"\n",
    "\n",
    "task = XLMRobertaForDocClassificationTask(\n",
    "    train_data_path=tiny_data_path,\n",
    "    valid_data_path=tiny_data_path,\n",
    "    test_data_path=tiny_data_path,\n",
    "    column_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    label_vocab=[\"0\", \"1\"],\n",
    "    model_name=\"xlm_roberta_dummy\",\n",
    "    epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T17:59:20.034520Z",
     "start_time": "2020-05-03T17:58:46.921102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 1.5845496833324433\n"
     ]
    }
   ],
   "source": [
    "task.prepare()\n",
    "task.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T17:59:20.930751Z",
     "start_time": "2020-05-03T17:59:20.821151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_labels: tensor([0]), \tscores: tensor([[-0.6255, -1.0933]], grad_fn=<LogSigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_inputs=[{\"text\": \"hello world\"}]\n",
    "prediction_labels, scores = task.predict(batch_inputs)\n",
    "print(f\"prediction_labels: {prediction_labels}, \\tscores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T03:43:24.486207Z",
     "start_time": "2020-05-02T03:43:24.376882Z"
    }
   },
   "source": [
    "## Fine tune a XLM-Roberta from scratch\n",
    "* construct transforms, which must be consistent with the ones used during pre-training. (We may load them from the Hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:54:52.524216Z",
     "start_time": "2020-05-03T18:54:46.520450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [Tokens(values=['▁hell', 'o', '▁world'], start_idxs=[0, 4, 6], end_idxs=[4, 5, 11])]\n",
      "\n",
      "model_inputs: {'tokens': tensor([[    0, 33600,    31,  8999,     2]]), 'pad_mask': tensor([[1, 1, 1, 1, 1]]), 'segment_labels': tensor([[0, 0, 0, 0, 0]]), 'positions': tensor([[0, 1, 2, 3, 4]])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pytext.contrib.pytext_lib import transforms\n",
    "\n",
    "\n",
    "tokenizer_transform = transforms.SpmTokenizerTransform()\n",
    "\n",
    "vocab = transforms.build_vocab(\"manifold://nlp_technologies/tree/xlm/models/xlm_r/vocab\")\n",
    "vocab_transform = transforms.VocabTransform(vocab)\n",
    "\n",
    "cap_transform = transforms.CapTransform(vocab.get_bos_index(), vocab.get_eos_index(), max_seq_len=256)\n",
    "\n",
    "transform_list = [tokenizer_transform, vocab_transform, cap_transform]\n",
    "\n",
    "input_transform = transforms.RobertaInputTransform(transform_list, 1, \"text\")\n",
    "\n",
    "batch_inputs = [{\"text\": \"hello world\"}]\n",
    "text_inputs = input_transform.extract_inputs(batch_inputs)\n",
    "print(f\"tokens: {tokenizer_transform(text_inputs)}\\n\")\n",
    "\n",
    "model_inputs = input_transform(text_inputs)\n",
    "print(f\"model_inputs: {model_inputs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:53:13.311740Z",
     "start_time": "2020-05-03T18:53:13.237136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1])}"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "bento_obj_id": "140388904985856"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytext.data.utils import Vocabulary\n",
    "\n",
    "label_vocab = Vocabulary([\"0\", \"1\"])\n",
    "label_transform = transforms.LabelTransform(\n",
    "    label_vocab, field_name=\"label\", pad_idx=-1\n",
    ")\n",
    "\n",
    "batch_inputs = [{\"text\": \"hello world\", \"label\": \"1\"}]\n",
    "label_transform(batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load pre-trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:53:25.559255Z",
     "start_time": "2020-05-03T18:53:24.582205Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytext.contrib.pytext_lib import models\n",
    "\n",
    "model = models.xlm_roberta_dummy_binary_doc_classifier(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* predict with a batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:55:30.717649Z",
     "start_time": "2020-05-03T18:55:30.591344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_inputs: {'tokens': tensor([[    0, 33600,    31,  8999,     2]]), 'pad_mask': tensor([[1, 1, 1, 1, 1]]), 'segment_labels': tensor([[0, 0, 0, 0, 0]]), 'positions': tensor([[0, 1, 2, 3, 4]])}\n",
      "\n",
      "logits: tensor([[0.0674, 0.2159]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "prediction_labels: tensor([1]), \tscores: tensor([[-0.6600, -0.5910]], grad_fn=<LogSigmoidBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = [{\"text\": \"hello world\"}]\n",
    "text_inputs = input_transform.extract_inputs(batch_inputs)\n",
    "\n",
    "model_inputs = input_transform(text_inputs)\n",
    "print(f\"model_inputs: {model_inputs}\\n\")\n",
    "\n",
    "logits = model(model_inputs)\n",
    "print(f\"logits: {logits}\\n\")\n",
    "\n",
    "prediction_labels, scores = model.get_pred(logits)\n",
    "print(f\"prediction_labels: {prediction_labels}, \\tscores: {scores}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get loss for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:01:58.631141Z",
     "start_time": "2020-05-03T19:01:58.518229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2587757110595703, logits: tensor([[-0.9213, -0.4178]], grad_fn=<AddmmBackward>), targets: {'label': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = [{\"text\": \"hello world\", \"label\": \"1\"}]\n",
    "text_inputs = input_transform.extract_inputs(batch_inputs)\n",
    "model_inputs = input_transform(text_inputs)\n",
    "logits = model(model_inputs)\n",
    "\n",
    "targets = label_transform(batch_inputs)\n",
    "loss = model.get_loss(logits, targets[\"label\"])\n",
    "print(f\"loss: {loss}, logits: {logits}, targets: {targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:26:04.123313Z",
     "start_time": "2020-05-03T18:26:03.767868Z"
    }
   },
   "source": [
    "* setup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:06:17.806814Z",
     "start_time": "2020-05-03T19:06:17.441417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[     0,    581,     57,   3632,      7,  66397,     70,  12562,   6659,\n",
      "          34735,    111,     70,  13950,      7,      5,      2],\n",
      "        [     0,    581,  57888,      7,   7228,     70,      6, 131803, 115851,\n",
      "            645,     70,   5452,   4293,      5,      2,      1]]), 'pad_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'segment_labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'positions': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0]]), 'label': tensor([1, 1])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytext.contrib.pytext_lib.datasets import TsvDataset\n",
    "\n",
    "\n",
    "tiny_data_path = \"/mnt/vol/pytext/users/stevenliu/glue_data/CoLA/dev_10.tsv\"\n",
    "\n",
    "train_dataset = TsvDataset(\n",
    "    file_path=tiny_dataset,\n",
    "    batch_size=2,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "valid_dataset = TsvDataset(\n",
    "    file_path=tiny_dataset,\n",
    "    batch_size=2,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "test_dataset = TsvDataset(\n",
    "    file_path=tiny_dataset,\n",
    "    batch_size=2,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "valid_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write your own train loop. e.g. SimpleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T19:09:35.851355Z",
     "start_time": "2020-05-03T19:09:16.384103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.7677797436714172\n",
      "Epoch: 1, Train Loss: 0.7366044402122498\n"
     ]
    }
   ],
   "source": [
    "from pytext.contrib.pytext_lib.trainers import SimpleTrainer\n",
    "from pytext.fb.optimizer import FairSeqAdam\n",
    "\n",
    "\n",
    "optimizer = FairSeqAdam(\n",
    "    model.parameters(),\n",
    "    lr=0.00001,\n",
    "    betas=[0.9, 0.999],\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    amsgrad=False,\n",
    ")\n",
    "\n",
    "trainer = SimpleTrainer()\n",
    "\n",
    "trainer.train(\n",
    "    dataloader=train_dataloader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "1621097908038238"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200427-000233",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/stevenliu/fbsource/fbcode/bento/kernels/local/pytext_lib_stevenliu/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "260268",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "1477464012436453",
   "tags": "",
   "tasks": "",
   "title": "xlm_roberta_for_doc_classification"
  },
  "kernelspec": {
   "display_name": "pytext_lib_stevenliu (local)",
   "language": "python",
   "name": "pytext_lib_stevenliu_local"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
