{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytext Library Prototype: xlm_roberta_for_doc_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T03:33:43.682359Z",
     "start_time": "2020-05-05T03:33:43.470294Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T06:19:57.817438Z",
     "start_time": "2020-05-05T06:19:57.741239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb\n",
    "import pytext.fb  # monkey patch for internal. e.g. manifold handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune a XLM-Roberta with a task\n",
    "* The task provides high-level APIs and it builds data, model, trainer, metric for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T03:34:07.140420Z",
     "start_time": "2020-05-05T03:34:07.062805Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytext.contrib.pytext_lib.tasks import XLMRobertaForDocClassificationTask\n",
    "\n",
    "\n",
    "tiny_data_path = \"/mnt/vol/pytext/users/stevenliu/glue_data/CoLA/dev_10.tsv\"\n",
    "\n",
    "task = XLMRobertaForDocClassificationTask(\n",
    "    train_data_path=tiny_data_path,\n",
    "    valid_data_path=tiny_data_path,\n",
    "    test_data_path=tiny_data_path,\n",
    "    column_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    label_vocab=[\"0\", \"1\"],\n",
    "    model_name=\"xlm_roberta_dummy\",\n",
    "    epoch=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T03:34:56.417450Z",
     "start_time": "2020-05-05T03:34:07.142604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 1.5804810762405395\n",
      "Epoch: 1, Train Loss: 1.6011629581451416\n"
     ]
    }
   ],
   "source": [
    "task.prepare()\n",
    "task.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T04:10:43.378584Z",
     "start_time": "2020-05-05T03:34:56.419832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_labels: tensor([0]), \tscores: tensor([[-0.7977, -1.1323]], grad_fn=<LogSigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_inputs=[{\"text\": \"hello world\"}]\n",
    "prediction_labels, scores = task.predict(batch_inputs)\n",
    "print(f\"prediction_labels: {prediction_labels}, \\tscores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T03:43:24.486207Z",
     "start_time": "2020-05-02T03:43:24.376882Z"
    }
   },
   "source": [
    "## Fine tune a XLM-Roberta from scratch\n",
    "* construct transforms, which must be consistent with the ones used during pre-training. (We may load them from the Hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T04:10:48.914619Z",
     "start_time": "2020-05-05T04:10:43.380924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: [Tokens(values=['▁hell', 'o', '▁world'], start_idxs=[0, 4, 6], end_idxs=[4, 5, 11])]\n",
      "\n",
      "model_inputs: {'tokens': tensor([[    0, 33600,    31,  8999,     2]]), 'pad_mask': tensor([[1, 1, 1, 1, 1]]), 'segment_labels': tensor([[0, 0, 0, 0, 0]]), 'positions': tensor([[0, 1, 2, 3, 4]])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pytext.contrib.pytext_lib import transforms\n",
    "\n",
    "\n",
    "# find available transforms with auto completion in IDE/Notebook by typing \"transforms.\" and tab\n",
    "tokenizer_transform = transforms.SpmTokenizerTransform()\n",
    "\n",
    "vocab = transforms.build_vocab(\"manifold://nlp_technologies/tree/xlm/models/xlm_r/vocab\")\n",
    "vocab_transform = transforms.VocabTransform(vocab)\n",
    "\n",
    "cap_transform = transforms.CapTransform(vocab.get_bos_index(), vocab.get_eos_index(), max_seq_len=256)\n",
    "\n",
    "transform_list = [tokenizer_transform, vocab_transform, cap_transform]\n",
    "\n",
    "input_transform = transforms.RobertaInputTransform(transform_list, 1, \"text\")\n",
    "\n",
    "batch_inputs = [{\"text\": \"hello world\"}]\n",
    "text_inputs = input_transform.extract_inputs(batch_inputs)\n",
    "print(f\"tokens: {tokenizer_transform(text_inputs)}\\n\")\n",
    "\n",
    "model_inputs = input_transform(text_inputs)\n",
    "print(f\"model_inputs: {model_inputs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T04:10:49.001352Z",
     "start_time": "2020-05-05T04:10:48.916845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "bento_obj_id": "140286004498176"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytext.data.utils import Vocabulary\n",
    "\n",
    "label_vocab = Vocabulary([\"0\", \"1\"])\n",
    "label_transform = transforms.LabelTransform(\n",
    "    label_vocab, field_name=\"label\", pad_idx=-1\n",
    ")\n",
    "\n",
    "batch_inputs = [{\"text\": \"hello world\", \"label\": \"1\"}]\n",
    "label_transform(batch_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load pre-trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T04:10:51.393839Z",
     "start_time": "2020-05-05T04:10:49.004515Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytext.contrib.pytext_lib import models\n",
    "\n",
    "# find available transforms with auto completion in IDE/Notebook by typing \"models.\" and tab\n",
    "model = models.xlm_roberta_dummy_binary_doc_classifier(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* predict with a batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-05T03:32:33.593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_inputs: {'tokens': tensor([[    0, 33600,    31,  8999,     2]]), 'pad_mask': tensor([[1, 1, 1, 1, 1]]), 'segment_labels': tensor([[0, 0, 0, 0, 0]]), 'positions': tensor([[0, 1, 2, 3, 4]])}\n",
      "\n",
      "logits: tensor([[0.7910, 0.1671]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "prediction_labels: tensor([0]), \tscores: tensor([[-0.3739, -0.6131]], grad_fn=<LogSigmoidBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = [{\"text\": \"hello world\"}]\n",
    "text_inputs = input_transform.extract_inputs(batch_inputs)\n",
    "\n",
    "model_inputs = input_transform(text_inputs)\n",
    "print(f\"model_inputs: {model_inputs}\\n\")\n",
    "\n",
    "logits = model(model_inputs)\n",
    "print(f\"logits: {logits}\\n\")\n",
    "\n",
    "prediction_labels, scores = model.get_pred(logits)\n",
    "print(f\"prediction_labels: {prediction_labels}, \\tscores: {scores}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get loss for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-05T03:32:33.597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.486728549003601, logits: tensor([[0.6509, 0.6617]], grad_fn=<AddmmBackward>), targets: {'label': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "batch_inputs = [{\"text\": \"hello world\", \"label\": \"1\"}]\n",
    "text_inputs = input_transform.extract_inputs(batch_inputs)\n",
    "model_inputs = input_transform(text_inputs)\n",
    "logits = model(model_inputs)\n",
    "\n",
    "targets = label_transform(batch_inputs)\n",
    "loss = model.get_loss(logits, targets[\"label\"])\n",
    "print(f\"loss: {loss}, logits: {logits}, targets: {targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-03T18:26:04.123313Z",
     "start_time": "2020-05-03T18:26:03.767868Z"
    }
   },
   "source": [
    "* setup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-05T03:32:33.602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([[     0,    581,     57,   3632,      7,  66397,     70,  12562,   6659,\n",
      "          34735,    111,     70,  13950,      7,      5,      2],\n",
      "        [     0,    581,  57888,      7,   7228,     70,      6, 131803, 115851,\n",
      "            645,     70,   5452,   4293,      5,      2,      1]]), 'pad_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'segment_labels': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'positions': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0]]), 'label': tensor([1, 1])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytext.contrib.pytext_lib.datasets import TsvDataset\n",
    "\n",
    "\n",
    "tiny_data_path = \"/mnt/vol/pytext/users/stevenliu/glue_data/CoLA/dev_10.tsv\"\n",
    "\n",
    "train_dataset = TsvDataset(\n",
    "    file_path=tiny_data_path,\n",
    "    batch_size=2,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "valid_dataset = TsvDataset(\n",
    "    file_path=tiny_data_path,\n",
    "    batch_size=2,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "test_dataset = TsvDataset(\n",
    "    file_path=tiny_data_path,\n",
    "    batch_size=2,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "valid_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=None)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write your own train loop. e.g. SimpleTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-05T03:32:33.606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.7989357113838196\n",
      "Epoch: 1, Train Loss: 0.7629225134849549\n"
     ]
    }
   ],
   "source": [
    "from pytext.contrib.pytext_lib.trainers import SimpleTrainer\n",
    "from pytext.fb.optimizer import FairSeqAdam\n",
    "\n",
    "\n",
    "optimizer = FairSeqAdam(\n",
    "    model.parameters(),\n",
    "    lr=0.00001,\n",
    "    betas=[0.9, 0.999],\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    amsgrad=False,\n",
    ")\n",
    "\n",
    "trainer = SimpleTrainer()\n",
    "\n",
    "trainer.train(\n",
    "    dataloader=train_dataloader,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epoch=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Custom batcher (e.g. shuffling, sorting by sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T05:56:02.692130Z",
     "start_time": "2020-05-05T05:56:02.206929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized: tensor([[    0,   581,    57,  3632,     7, 66397,    70, 12562,  6659, 34735,\n",
      "           111,    70, 13950,     7,     5,     2]])\n",
      "batched: [tensor([    0, 23213, 44632,     7,    47,    70,  6524,    67,  7844, 51952,\n",
      "            4,  2412, 62163,     8, 11856,   297,     5,     2]), tensor([    0,   581,    57,  3632,     7, 66397,    70, 12562,  6659, 34735,\n",
      "          111,    70, 13950,     7,     5,     2]), tensor([     0,    581,  57888,      7,   7228,     70,      6, 131803, 115851,\n",
      "           645,     70,   5452,   4293,      5,      2]), tensor([    0,    87, 35968,   450,    70,  1286,  4939, 73203,     4,    70,\n",
      "         1286,   764, 19859,     5,     2]), tensor([    0,   581,   348, 25388, 23213,  4163,     4,    70,  1286,  2412,\n",
      "        54811,    99, 75073,     5,     2]), tensor([     0,    581, 135969,    289,     54,   1181,    148,  10384,  32502,\n",
      "         68034,   5078,    184,      5,      2])]\n",
      "length of tokens for each row in a batch: [18, 16, 15, 15, 15, 14]\n"
     ]
    }
   ],
   "source": [
    "from pytext.contrib.pytext_lib.datasets import TsvDataset\n",
    "from pytext.contrib.pytext_lib.datasets import PoolingBatcher, NestedDataset\n",
    "\n",
    "\n",
    "tiny_data_path = \"/mnt/vol/pytext/users/stevenliu/glue_data/CoLA/dev_10.tsv\"\n",
    "\n",
    "# apply transforms to each row and skip batching by setting batch size=1\n",
    "tokenized_dataset = TsvDataset(\n",
    "    file_path=tiny_data_path,\n",
    "    batch_size=1,\n",
    "    field_names=[\"dummy1\", \"label\", \"dummy2\", \"text\"],\n",
    "    transform=input_transform,\n",
    "    label_transform=label_transform,\n",
    ")\n",
    "\n",
    "batches = [batch for batch in tokenized_dataset]\n",
    "print(f\"tokenized: {batches[0]['tokens']}\")\n",
    "\n",
    "# apply custom Batcher and skip transforms\n",
    "# sort by length of tokens\n",
    "sort_key=lambda row: len(row['tokens'])\n",
    "\n",
    "batched_dataset = NestedDataset(\n",
    "    dataset=tokenized_dataset,\n",
    "    batcher=PoolingBatcher(batch_size=6, pool_num_batches=5, sort_key=sort_key),\n",
    ")\n",
    "\n",
    "batches = [batch for batch in batched_dataset]\n",
    "print(f\"batched: {repr(batches[0]['tokens'])}\")\n",
    "\n",
    "sequence_length_in_a_batch = [len(t) for t in batches[0]['tokens']]\n",
    "print(f\"length of tokens for each row in a batch: {sequence_length_in_a_batch}\")"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "disseminate_notebook_id": {
   "notebook_id": "1621097908038238"
  },
  "disseminate_notebook_info": {
   "bento_version": "20200504-000300",
   "description": "",
   "hide_code": false,
   "hipster_group": "",
   "kernel_build_info": {
    "error": "The file located at '/data/users/stevenliu/fbsource/fbcode/bento/kernels/local/pytext_lib_stevenliu/TARGETS' could not be found."
   },
   "no_uii": true,
   "notebook_number": "260268",
   "others_can_edit": false,
   "reviewers": "",
   "revision_id": "254431932276796",
   "tags": "",
   "tasks": "",
   "title": "xlm_roberta_for_doc_classification"
  },
  "kernelspec": {
   "display_name": "pytext_lib_stevenliu (local)",
   "language": "python",
   "name": "pytext_lib_stevenliu_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
